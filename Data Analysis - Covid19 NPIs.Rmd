
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
#install.packages("xgboost")
loadlibs = function(libs) {
  for(lib in libs) {
    class(lib)
    if(!do.call(require,as.list(lib))) {install.packages(lib)}
    do.call(require,as.list(lib))
  }
}
libs = c("tidyverse","data.table","stargazer", "caret", "e1071",
         "randomForest", "C50", "xgboost", "ggplot2", "cowplot")
loadlibs(libs)
```

## Load Data Set

```{r}
#Load dataset
working_df = read.csv("working_14days.csv") %>% 
  filter(submission_date >= as.Date("2020-03-01") & submission_date < as.Date("2020-07-01"))

#Covid deaths outcome variable df
deaths_df = working_df %>% 
  select(new_cases_per_100k, submission_date, state, c(85:92) )
#Check  the df structure
str(deaths_df)
#Convert the policy indicators to factors:
deaths_df = deaths_df %>% 
  mutate_if(is.integer, as.factor)
```
## Control Merges
1. Can we simplify the age distribution? maybe just proportion over 60? 
2. Race - how do we want to incorporate? proportion minorities? 
3. Need to add median income
4. I think We discussed using population density instead of population?
5. Not sure we gender is going to give us much. Maybe remove? 
6. We should add some sort of health indicators - Ahmed might have something in mind? otherwise we can use 

## Split and prepare train/test datasets

```{r}
set.seed(1)
#generate random index list for training and test sets
trainIndex = sample(1:nrow(deaths_df), 0.8 * nrow(deaths_df))
#create train set 
trainDeaths = deaths_df[trainIndex, ]
#create test set
testDeaths = deaths_df[-c(trainIndex), ]
```

```{r}
# convert features to matrix 
trainMTX=model.matrix(new_cases_per_100k~., trainDeaths)[, -1]
testMTX=model.matrix(new_cases_per_100k~., testDeaths)[, -1]
```

## Correlation Plot - for continuous variables

```{r, fig.width=12, fig.height=10}
#should run once we add continuous control variables

``` 

#Variable Frequency - for categorical variables 
```{r}
#Should add)
```

# Modelling 

Could start with Lasso? For feature selection...

## Model 2. Random Forest

Next, a tree-based model is applied for this classification analysis. 

```{r}
#Define random grid parameters for tuning the models
tc = trainControl(method = "repeatedcv",
                         number = 10, 
                         search ="grid")

tg = expand.grid(.mtry= c(1,2,3,4))
``` 

```{r}
#create empty data frame to store each of the models in: 
modellist = list()

#Train the Model
for (ntree in c(1000,1500,2000,2500)){
  rfModel = train(new_cases_per_100k ~ . -submission_date -state,
                  trainDeaths,
                  method = "rf",
                  metric = "RMSE",
                  trControl = tc,
                  tuneGrid = tg)
  key <- toString(ntree)
  modellist[[key]] <- rfModel
}
```

```{r}
#Create training results data frame
trainResults = data.frame(
  ntrees = rep(c(1000, 1500, 2000, 2500), each = 4), 
  mtry = c("1","2","3","4"), 
  RMSE = c(modellist$"1000"$results[,"RMSE"],
           modellist$"1500"$results[,"RMSE"],
           modellist$"2000"$results[,"RMSE"],
           modellist$"2500"$results[,"RMSE"])
)

#plot training results
ggplot(trainResults, aes(x = ntrees, y = RMSE)) + 
  geom_line(aes(color = mtry)) + 
  geom_point(aes(color = mtry))
```

```{r}
#Predict results using ntre = 2500 and mtry = 4
rfPred = predict(modellist$"2500", newdata = testDeaths)
rfRMSE = sqrt(mean ( (testDeaths$new_cases_per_100k - rfPred) ^ 2 ) )
```

Questions: 
1. training data? is the model automatically bagging? 
2. in the context of a regression tree, what do the control variables mean?
3. Should we be creating a validation set for model selection? Separately from a test set for model evaluation? 

## Model 3. XGBoost

```{r}
#Convert the predictor variable matrices to xgb.DMatrix data types
xgb_train=xgb.DMatrix(trainMTX)
xgb_test=xgb.DMatrix(testMTX)

#Define random grid parameters for tuning the models
XGBtc=trainControl(
  method='cv',
  number=5,
  allowParallel=TRUE,
  verboseIter=FALSE,
  returnData=FALSE
)
# create grid for tuning hyperparameters
XGBtg <- expand.grid(nrounds = c(20, 50, 100),
                       max_depth = c(5, 10, 15),
                       colsample_bytree = seq(0.5, 0.9, length.out = 5),
                       eta = c(0.1, 0.2, 0.4),
                       gamma=c(0, 2, 3),
                       min_child_weight = 1,
                       subsample = 0.75
                      )

```

```{r warning = FALSE}
# train model 
xgb_model = train(
  xgb_train, trainDeaths$new_cases_per_100k,  
  trControl = XGBtc,
  tuneGrid = XGBtg,
  method = "xgbTree")
```


```{r}
# get hyperparameters of best model from grid search 
xgb_model$bestTune
```


```{r}
XGBPred=predict(xgb_model, testMTX)
XGBRMSE = sqrt(mean ((testDeaths$new_cases_per_100k - XGBPred) ^ 2 ) )
```

1. If XGBoost is performing this well on unseen data- do we need to add more variables? 