---
title: "Feature_Select_RF"
author: "ClareCallahan"
date: "4/17/2021"
output: html_document
---

```{r setup, include=FALSE}
loadlibs = function(libs) {
  for(lib in libs) {
    class(lib)
    if(!do.call(require,as.list(lib))) {install.packages(lib)}
    do.call(require,as.list(lib))
  }
}
libs = c("tidyverse","data.table","stargazer", "knitr", "shiny",
         "tidymodels", "lubridate", "rpart.plot", "caret", "e1071",
         "readxl", "fuzzyjoin", "mgcv", "randomForest", "glmnet")
loadlibs(libs)
```

```{r}
setwd("~/GitHub/Covid-19-Closure-Impact")

#df<-read.csv('Covid_Pred.csv')

df<- read.csv('shiny_merged_dataset_example.csv')
  names(df) <-tolower(names(df))
  
colnames(df)[which(names(df) == "covid_measure")] <- "y"
  
  drops<-c( "x") #droping 
df <-df[, -which(names(df) %in% drops)]
  
 df<- df%>%
  mutate_at(c('date'), ~ as.Date(., "%Y-%m-%d")) %>% 
  mutate(two_week_forecast_date = date + 14)%>% 
  mutate_if(is.character, as.factor)

  
  # Get latest date 
latest_date = max(df$date)
#Two week outcome 
twoWeek_df = df %>% 
  select(date, state, y) %>% 
  rename(two_week_outcome = y,
         two_week_forecast_date = date) 
#Merge with original to create a working df
working_df = df %>% 
  left_join(twoWeek_df,
            by = c("state" = "state",
                   "two_week_forecast_date" = "two_week_forecast_date")) %>% 
  mutate(year = as.factor(year(two_week_forecast_date)),
         week = as.factor(week(two_week_forecast_date)))



working_df[is.na(working_df)] <-0

#y = covid_measure
colnames(working_df)[which(names(working_df) == "y")] <- "two_week_backcast"
colnames(working_df)[which(names(working_df) == "two_week_outcome")] <- "y"

#names(df)[names(df) == "Outcome.Variable"] <- "original_y" #creating consitency 
#names(df)[names(df) == "Other"] <- "Other_Race"  #"Other is not meaningful in output

```



```{r}
##Train/Test split 
#n<-lm_feats%>% count() 
#n<-as.numeric(n/2)
#data2 = lm_feats[sample(1:nrow(lm_feats)),]  # permute rows


#Create train set
train_df = function(df) {
  #Call addForecastCols function to create the outcome variable
  #create variable for latest date for which two week covid measure is avaiable
  latest_date = max(df$date)
  #Create a training set - reserving the last two weeks with obvserved 
  #two_week__outcome are reserved as a test set. 
  train = df[df$two_week_forecast_date <= (latest_date - 14), ]
  return(train)
}

  latest_date = max(glm_feats$date)
  #Create a training set - reserving the last two weeks with obvserved 
  #two_week__outcome are reserved as a test set. 
  train = glm_feats[glm_feats$two_week_forecast_date <= (latest_date - 14), ]

#Create test set
test_df = function(df) {
  #create variable for latest date for which two week covid measure is avaiable
  latest_date = max(df$date)
  #Create a test set - using the last two weeks for which two_week_outcom is observed. 
  test = df[(df$two_week_forecast_date <= latest_date) &
                      (df$two_week_forecast_date > (latest_date - 14)), ]
  return(test)
}


#Create prediction set - for all states
pred_df = function(df) {
  #Call addForecastCols function to create the outcome variable
  #create variable for latest date for which two week covid measure is avaiable
  latest_date = max(df$date)
  #Create a prediction set - using the  two weeks for which two_week_outcom is not observed. 
  pred = df[df$two_week_forecast_date > latest_date +7, ]
  return(pred)
}




```


```{r}
#glm_feats$date<-working_df$date   
#glm_feats<- glm_feats%>% mutate_at(c('date'), ~ as.Date(., "%Y-%m-%d"))
#glm_feats$glm_feats$two_week_forecast_date <- working_df$

#forecast_col<-addForecastCols(glm_feats)
datatrain<-train_df(working_df)
datatest<-test_df(working_df)
datapreds<- pred_df(working_df)

drops<-c("year","week","two_week_forecast_date")

#datatest$two_week_forecast_date
datatrain <-datatrain[, -which(names(datatrain) %in% drops)]
datatest <- datatest[, -which(names(datatest) %in% drops)]
datapreds <- datapreds[, -which(names(datapreds) %in% drops)]
#datacleaning for randomforest 
```


```{r}

drops<-c( "year", "week", "two_week_forecast_date") #droping 
working_df <-working_df[, -which(names(working_df) %in% drops)]
##Moving Ouctome Variable to front of dataset for ease of splitting
col_idx <- grep("^y$", names(working_df))
working_df<- working_df[, c(col_idx, (1:ncol(working_df))[-col_idx])]



```
##Split
```{r}

end<-ncol(working_df)
x <- working_df[,2:end]
y <- working_df[,1]

#Removing Y from the tuning paratmers 

```

## Tuning
The following is tuning the number of varaibles reandomple sampled as candiate at each split (mtry)
```{r}

# Algorithm Tune (tuneRF)
set.seed(10)
bestmtry <- tuneRF(x, y, stepFactor=1.5, improve=1e-5, ntree=500)
print(bestmtry)
bestmtry_var<- bestmtry[1,1]

```


```{r}

r.forest <- function(working_df){
 
  rf <- randomForest(y ~ ., mtry= bestmtry_var, 
  data=working_df)

  varImpPlot(rf,sort=TRUE, n.var=min(10, nrow(rf$importance)),
           type=NULL, class=NULL, scale=TRUE, 
           main=deparse(substitute(rf))) 
  
  features <- as.data.frame(rf$importance)
  features <-as.data.frame(setNames(cbind(rownames(features), features, row.names= NULL), c("Feature", "NodPurity"))) 
  top<- top_n(features, 10, features$NodPurity)
  top_f<-as.vector(top$Feature)

  return(top_f)
}
 

rf.tops <-function(sample_df, top_f){
  new<-sample_df%>%
  select(all_of(c(top_f)))
  
  new$y<-sample_df$y

  return(new)

}

#glment select levels of sparcity....  

```

```{r}

tops <-r.forest(working_df)
```

###Start of GLMNET
```{r}
datatrain<-rf.tops(datatrain, tops)
datatest<-rf.tops(datatest,tops)
datapreds<- rf.tops(datapreds,tops)

```


###Geocoding 


#Data from: https://public.opendatasoft.com/explore/dataset/us-zip-code-latitude-and-longitude/export/ 
Process needed to create 2D for 'state' to be incorporated into GLM with poly option 


#Geotagging for glmnet
```{r}

#setwd("~/GitHub/Covid-19-Closure-Impact/Data")
geo_join<- function(df){
geo<- read.csv('us-zip-code-latitude-and-longitude.csv', header= FALSE, stringsAsFactors = FALSE)

geo<- as.data.frame(geo)

geo<-geo%>% separate(V1, into=c("Zip", "City", "State", "Latitude","Longitude","Timezone","Daylight_savings", "geopoint"), sep=";" )

geo<- geo[-1,]

geo$Latitude<-as.numeric(geo$Latitude)
geo$Longitude<-as.numeric(geo$Longitude)

names(geo) <-tolower(names(geo))
  
state_latLong <- geo%>%
  group_by(state)%>%
  summarise(latitude = mean(latitude, na.rm = TRUE),
            longitude = mean(longitude, na.rm = TRUE))
  return(left_join(df, state_latLong, by='state'))
}
```


#Joining Long/Lat data and selected features
```{r}
# this step should be moved to data cleaning, ask if their data is by state or county, county can be a direct match to the original dataset 
datatrain_lm<- geo_join(datatrain)
datatest_lm<- geo_join(datatest)
datapreds_lm<-geo_join(datapreds)
#id<-rownames(features)
#features<-cbind(id=id, features)

```


#cleaning on test/train/ preds 
```{r}

cleaning_lm<- function(df){
df1<-df[, -which(names(df) %in% c("state"))]#%>% 
  #select(-1) #removing 'id'

col_idx <- grep("^y$", names(df1))
df1 <- df1[, c(col_idx, (1:ncol(df1))[-col_idx])]

#dropping date, cannot create a poly w/ cbind() after polys created
date_hold<- as.data.frame(df1$date)
colnames(date_hold)<-"date"
#two_week_forecast_date<-as.data.frame(features1$two_week_forecast_date)
drops<-c( "date") #droping 
df1<-df1[, -which(names(df1) %in% drops)]

 feats<- function(xf, column_numbers) {

   xf %>% 
    mutate_at(vars(column_numbers), funs(sqr = (.)^2))%>%
   mutate_at(vars(column_numbers), funs(cube =(.)^3))
   }
end<-ncol(features1)

#, two_week_forecast_date
return(cbind(feats(df1, 2:end), date_hold)) #do not include y or "id" in poly calcs
}
#one<- as.character(top[1,1])

#typeof(features1$two_week_prior)

```

```{r}
datatrain_poly<- cleaning_lm(datatrain_lm)
datatest_poly<- cleaning_lm(datatest_lm)
datapreds_poly<-cleaning_lm(datapreds_lm)

```


##Glmnet 
```{r}

datatrain_glmnet<- datatrain_poly %>%
mutate(across(where(is.factor), ~ fct_lump_lowfreq(.))) %>%
model.matrix(object= ~ .-1, . , contrasts.arg =
                                                 lapply(data.frame(.[,sapply(data.frame(.), is.factor)]),
                                                        contrasts, contrasts = FALSE))

datatest_glmnet<- datatest_poly %>%
mutate(across(where(is.factor), ~ fct_lump_lowfreq(.))) %>%
model.matrix(object= ~ .-1, . , contrasts.arg =
                                                                lapply(data.frame(.[,sapply(data.frame(.), is.factor)]),
                                                        contrasts, contrasts = FALSE))

datapreds_glmnet<- datapreds_poly%>%
mutate(across(where(is.factor), ~ fct_lump_lowfreq(.))) %>%
model.matrix(object= ~ .-1, . , contrasts.arg =
                                                                lapply(data.frame(.[,sapply(data.frame(.), is.factor)]),
                                                        contrasts, contrasts = FALSE))



train_glm<- glmnet(
  x = datatrain_glmnet[,-1],
  y = datatrain_glmnet[,1], relax = FALSE, nfolds= 3)


#plot(train_glm)

lamMin<- min(train_glm[["lambda"]])

test<-predict(train_glm, newx= datatest_glmnet[,-1], newy=datatest_glmnet[,1], s = lamMin , interval="confidence")

preds<- predict(train_glm, newx= datapreds_glmnet[,-1], newy=datapreds_glmnet[,1], s = lamMin , interval="confidence")
```
#RMSE testing 
```{r}

test<- as.data.frame(test) 
glm_test<- as.data.frame(datatest_glmnet)
preds_act<- cbind(test, glm_test$y)

RMSE_glm<- sqrt(mean(preds_act[,2]-preds_act[,1])^2)


```


###Plotting 

```{r}



```