---
title: "Feature_Select_RF"
author: "ClareCallahan"
date: "4/17/2021"
output: html_document
---

```{r setup, include=FALSE}
loadlibs = function(libs) {
  for(lib in libs) {
    class(lib)
    if(!do.call(require,as.list(lib))) {install.packages(lib)}
    do.call(require,as.list(lib))
  }
}
libs = c("tidyverse","data.table","stargazer", "knitr", "shiny",
         "tidymodels", "lubridate", "rpart.plot", "caret", "e1071",
         "readxl", "fuzzyjoin", "mgcv", "randomForest")
loadlibs(libs)
```

```{r}
setwd("~/GitHub/Covid-19-Closure-Impact")
#df<-read.csv('Covid_Pred.csv')
df<- read.csv('shiny_merged_dataset_example.csv')
#df$submission_date<-as.Date(df$submission_date, format="%m/%d/%Y") 
#df$submission_date<- as.Date(as.character(df$submission_date), format="%m/%d/%Y")
#Dropping the N/A columns as accounted for with the one hot encoding columns (and pervents RF)
df<- df[, colSums(is.na(df))==0]

df<- select(df, -X)
df<-droplevels(df)
str(df)

names(df)[names(df) == "Outcome.Variable"] <- "y" #creating consitency 
names(df)[names(df) == "Other"] <- "Other_Race"  #"Other is not meaningful in output

##Moving Ouctome Variable to front of dataset for ease of splitting
col_idx <- grep("y", names(df))
df <- df[, c(col_idx, (1:ncol(df))[-col_idx])]
head(df)
```
##Split

```{r}

end<-ncol(df)
x <- df[,2:end]
y <- df[,1]



```

## Tuning
The following is tuning the number of varaibles reandomple sampled as candiate at each split (mtry)
```{r}

# Algorithm Tune (tuneRF)
set.seed(10)
bestmtry <- tuneRF(x, y, stepFactor=1.5, improve=1e-5, ntree=500)
print(bestmtry)

bestmtry_var<- bestmtry[1,1]

```

#df1 = df %>%
 # mutate_if(is.character, as.factor) #%>% 
  #mutate_if(is.factor, forcats::fct_lump_min, min=100) 
#df1$new_cases<- as.factor(df1$new_cases)


t<- as.numeric(count(df)*.75)

set.seed(12345) 
df2 = df[sample(1:nrow(df)),]
train = df2[1:t,]
test = df2[-(1:t),]




control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(seed)
tunegrid <- expand.grid(.mtry=c(1:15))
rf_gridsearch <- train(Class~., data=dataset, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
 



```{r}
#did not end up using train & test as not testing accuracy of feature selection for the time being

#change to 'outcome'

rf <- randomForest(y ~ ., mtry= bestmtry_var, 
  data=df
)

varImpPlot(rf,sort=TRUE, n.var=min(10, nrow(rf$importance)),
           type=NULL, class=NULL, scale=TRUE, 
           main=deparse(substitute(rf))) 


features <- as.data.frame(rf$importance)
features <-as.data.frame(setNames(cbind(rownames(features), features, row.names= NULL), c("Feature", "NodPurity"))) 

top<- top_n(features, 10, features$NodPurity)
#toplist<- as.list(top$Feature) #list variable did not work in gam
one<- as.character(top[1,1])



gam_mod<-gam(y ~ s(gam one), data=df)

#pruning random forest 

#https://towardsdatascience.com/random-forest-in-r-f66adf80ec9
```

